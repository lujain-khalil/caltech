
============================================================
FILE: src\config.py
============================================================
# --- EXPERIMENT CONFIGURATIONS ---
ATTEMPT_NUM = 0

DEFAULT_SLO = 70
DEFAULT_RTT = 50
DEFAULT_BW = 15
DEFAULT_DATASET = "fmnist"
DEFAULT_SLOWDOWN = 50.0

# Default experiment parameters
DEFAULT_EPOCHS = 20
DEFAULT_BATCH_SIZE = 64
DEFAULT_LR = 0.001
DEFAULT_LAMBDA_LAT = 3.0
DEFAULT_MU = 15.0


============================================================
FILE: src\data_utils.py
============================================================
import torch
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

def get_dataloaders(dataset_name, batch_size):
    """
    Returns train_loader, test_loader, num_classes, input_channels
    Forces all inputs to 3 channels (RGB) and 32x32 resolution.
    """
    dataset_name = dataset_name.lower()
    
    # Transform for Grayscale -> Fake RGB
    transform_gray_to_rgb = transforms.Compose([
        transforms.Resize((32, 32)),
        transforms.Grayscale(num_output_channels=3), # DUPLICATES CHANNELS
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # 3-channel norm
    ])

    if dataset_name == "mnist":
        train_d = datasets.MNIST(root='./data', train=True, download=True, transform=transform_gray_to_rgb)
        test_d = datasets.MNIST(root='./data', train=False, download=True, transform=transform_gray_to_rgb)
        num_classes = 10

    elif dataset_name == "fmnist" or dataset_name == "fashionmnist":
        train_d = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform_gray_to_rgb)
        test_d = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform_gray_to_rgb)
        num_classes = 10
        
    elif dataset_name == "cifar10":
        # CIFAR is already 3 channels, 32x32
        transform_train = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        transform_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))
        ])
        train_d = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
        test_d = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
        num_classes = 10
        
    else:
        raise ValueError(f"Dataset {dataset_name} not supported.")

    train_loader = DataLoader(train_d, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_d, batch_size=batch_size, shuffle=False)
    
    # We now always return 3 channels
    return train_loader, test_loader, num_classes, 3

============================================================
FILE: src\deployment_model.py
============================================================
import torch
import torch.nn as nn
import torch.nn.functional as F

class DeploymentAwareResNet(nn.Module):
    def __init__(self, backbone, num_classes=10, exit_points=[1, 2, 3]):
        """
        backbone: The SplittableResNet18 instance
        exit_points: Indices of blocks after which we attach an early exit.
                     For ResNet18 (blocks 0-4), 1 and 3 are good standard spots.
        """
        super().__init__()
        self.backbone = backbone
        self.exit_points = set(exit_points)
        self.num_blocks = len(backbone.blocks)
        
        # 1. EARLY EXIT HEADS
        # We need to know the input size for the exit heads. 
        # For ResNet18: Block 1 -> 64 channels, Block 3 -> 256 channels.
        # We create a dictionary to hold these distinct heads.
        self.exit_heads = nn.ModuleDict()
        
        # Simple conv-based exit heads (Classifier + Confidence estimator)
        for idx in exit_points:
            in_channels = self._get_channels_for_block(idx)
            self.exit_heads[str(idx)] = nn.Sequential(
                nn.AdaptiveAvgPool2d((1, 1)),
                nn.Flatten(),
                nn.Linear(in_channels, num_classes)
            )
            
        # 2. SPLIT POINT LOGITS
        # We have K candidate split points (between blocks).
        # Logits z_1 ... z_K learn the preference for splitting at each point.
        self.split_candidates = [0, 1, 2, 3]
        self.split_logits = nn.Parameter(torch.zeros(len(self.split_candidates)))
        
        # 3. EXIT THRESHOLD SCALING (Learnable gamma from paper)
        # Gamma controls the sharpness of the sigmoid for exit probability
        self.exit_scale = nn.Parameter(torch.ones(len(exit_points)))
        self.exit_threshold = nn.Parameter(torch.empty(len(exit_points)).uniform_(0, 1))

    def _get_channels_for_block(self, idx):
        # Helper to hardcode channel sizes for standard ResNet18 blocks
        # Block 0: 64, Block 1: 64, Block 2: 128, Block 3: 256, Block 4: 512
        sizes = {0: 64, 1: 64, 2: 128, 3: 256, 4: 512}
        return sizes.get(idx, 512)

    def forward(self, x, temperature=1.0):
        """
        Returns:
            final_pred: Prediction from the very end of the network
            exit_preds: List of predictions from early exits
            split_probs: Softmax probabilities of splitting at each layer
            exit_probs: Probabilities of taking each early exit
        """
        
        # 1. Calculate Split Probabilities (Gumbel-Softmax) [cite: 133]
        # This makes the discrete choice "differentiable"
        split_probs = F.gumbel_softmax(self.split_logits, tau=temperature, hard=False, dim=0)
        
        exit_preds = {}
        exit_confidences = {}
        current_exit_idx = 0
        
        # 2. Forward Pass through Blocks
        for i, block in enumerate(self.backbone.blocks):
            x = block(x)
            
            # If this block has an early exit attached
            if i in self.exit_points:
                # Run the early exit head
                out_exit = self.exit_heads[str(i)](x)
                exit_preds[i] = out_exit
                
                # Calculate Confidence (Max Logit) [cite: 138]
                # We use max of softmax as a proxy for confidence
                softmax_out = F.softmax(out_exit, dim=1)
                confidence, _ = torch.max(softmax_out, dim=1)
                
                # Calculate Exit Probability p_i [cite: 140]
                # p_i = sigmoid( (Confidence - Threshold) * Scale )
                gamma = self.exit_scale[current_exit_idx]
                tau = self.exit_threshold[current_exit_idx]
                
                p_exit = torch.sigmoid((confidence - tau) * gamma)
                exit_confidences[i] = p_exit
                current_exit_idx += 1
        
        # Final prediction (Cloud)
        x_final = self.backbone.avgpool(x)
        x_final = torch.flatten(x_final, 1)
        final_pred = self.backbone.fc(x_final)
        
        return final_pred, exit_preds, split_probs, exit_confidences

============================================================
FILE: src\evaluate.py
============================================================
import torch
import torch.nn.functional as F
from torchvision import datasets, transforms
import numpy as np
from .data_utils import get_dataloaders
from . import config as Config

def evaluate_model(model, net_sim, profiles, device, 
                   dataset_name=Config.DEFAULT_DATASET, batch_size=Config.DEFAULT_BATCH_SIZE):
    """
    Simulates inference on the test set.
    
    Args:
        model: The deployment-aware model with early exits
        net_sim: Network simulator for bandwidth and latency
        profiles: Hardware profiles with timing information
        device: torch device to run on
        dataset_name: Dataset to test on ('cifar10', 'mnist', 'fmnist')
        batch_size: Batch size for testing
    """
    model.eval()
    
    # Load Test Data using the appropriate dataset
    dataset_name_lower = dataset_name.lower() if isinstance(dataset_name, str) else "fmnist"
    
    # Validate dataset choice
    valid_datasets = {"cifar10", "mnist", "fmnist"}
    if dataset_name_lower not in valid_datasets:
        print(f"Warning: Dataset '{dataset_name}' not recognized. Using 'fmnist' instead.")
        dataset_name_lower = "fmnist"
    
    print(f"Loading {dataset_name_lower.upper()} test dataset for evaluation...")
    
    # Get the correct test loader
    _, test_loader, _, _ = get_dataloaders(
        dataset_name=dataset_name_lower,
        batch_size=batch_size,
    )
    
    # Metrics
    total_samples = 0
    correct = 0
    latencies = []
    exit_counts = {k: 0 for k in model.exit_points}  # How many times did we exit at each point?
    exit_counts['final'] = 0

    # Track confidence values for samples that actually exit at each head
    exit_conf_values = {k: [] for k in model.exit_points}

    
    # 1. Determine the Fixed Split Point (The one with highest logit)
    split_probs = F.softmax(model.split_logits, dim=0)
    split_point = torch.argmax(split_probs).item()
    
    split_idx = torch.argmax(split_probs).item()
    split_block = model.split_candidates[split_idx]

    print(f"Running Evaluation on {dataset_name_lower.upper()}. Fixed Split Point: split_idx={split_idx}, block={split_block}")


    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            batch_curr_size = data.size(0)
            
            # Sample Network (Stochastic per batch, or fix it for deterministic testing)
            bw, rtt = net_sim.sample_network_state()
            
            # We must manually run the blocks to simulate the decision flow
            x = data
            exited_mask = torch.zeros(batch_curr_size, dtype=torch.bool, device=device)
            final_preds = torch.zeros(batch_curr_size, 10, device=device)
            
            # --- EDGE EXECUTION ---
            current_latency = torch.zeros(batch_curr_size, device=device)
            
            num_blocks = len(model.backbone.blocks)
            for i, block in enumerate(model.backbone.blocks):
                
                hw_key = f"block_{i}"
                if i <= split_block:
                    exec_time = profiles[hw_key]["edge_time_sec"]
                else:
                    exec_time = profiles[hw_key]["cloud_time_sec"]
                
                # 1. Run the block (only latency charged to survivors)
                current_latency[~exited_mask] += exec_time
                x = block(x)

                # Check Early Exit
                if i in model.exit_points:
                    # Run Exit Head
                    exit_out = model.exit_heads[str(i)](x)
                    
                    # Calculate Probability
                    confidence, _ = torch.max(F.softmax(exit_out, dim=1), dim=1)
                    
                    # Get Thresholds
                    ex_idx = sorted(list(model.exit_points)).index(i)
                    gamma = model.exit_scale[ex_idx]
                    tau = model.exit_threshold[ex_idx]
                    
                     # HARD DECISION: Exit if p > 0.5
                    p_exit = torch.sigmoid((confidence - tau) * gamma)
                    should_exit = (p_exit > 0.5) & (~exited_mask)
                    
                    if should_exit.any():
                        # Store predictions for those exiting now
                        final_preds[should_exit] = exit_out[should_exit]
                        
                        # Update counts
                        count = should_exit.sum().item()
                        exit_counts[i] += count

                        # Log confidences for those who actually exited here
                        exit_conf_values[i].extend(
                            confidence[should_exit].detach().cpu().tolist()
                        )

                        # Mark as exited
                        exited_mask = exited_mask | should_exit
                
                # Check if we crossed the split point -> Add Communication Latency
                if i == split_block and (split_block < num_blocks - 1):
                    # Calc Comm Time
                    data_size = profiles[f"block_{i}"]["output_bytes"]
                    t_comm = net_sim.estimate_transmission_time(data_size, bw, rtt)
                    
                    # Add comm time only to those who haven't exited yet
                    current_latency[~exited_mask] += t_comm

            # --- FINAL CLASSIFIER (Cloud) ---
            # If anyone is left
            if (~exited_mask).any():
                x_final = model.backbone.avgpool(x)
                x_final = torch.flatten(x_final, 1)
                out_final = model.backbone.fc(x_final)

                # Only assign predictions for the samples that haven't exited
                remaining = ~exited_mask
                final_preds[remaining] = out_final[remaining]
                exit_counts['final'] += remaining.sum().item()
            
            # Compute Accuracy
            preds = final_preds.argmax(dim=1)
            correct += preds.eq(target).sum().item()
            total_samples += batch_curr_size
            
            # Store latencies
            latencies.extend(current_latency.cpu().tolist())

    # Final Stats
    accuracy = 100. * correct / total_samples
    avg_latency = np.mean(latencies) * 1000 # to ms
    p95_latency = np.percentile(latencies, 95) * 1000
    
    # Exit rates (fraction of samples)
    exit_rates = {}
    for k, v in exit_counts.items():
        exit_rates[str(k)] = v / total_samples

    # Exit thresholds and scales per head
    sorted_exits = sorted(model.exit_points)
    exit_thresholds = {}
    exit_scales = {}
    for idx, block_idx in enumerate(sorted_exits):
        exit_thresholds[int(block_idx)] = float(model.exit_threshold[idx].item())
        exit_scales[int(block_idx)] = float(model.exit_scale[idx].item())

    # Confidence stats for samples that actually exited at each head
    exit_conf_stats = {}
    for k in model.exit_points:
        vals = exit_conf_values[k]
        if len(vals) > 0:
            exit_conf_stats[int(k)] = {
                "mean_conf_exit": float(np.mean(vals)),
                "min_conf_exit": float(np.min(vals)),
                "max_conf_exit": float(np.max(vals)),
                "num_exited": int(len(vals))
            }
        else:
            exit_conf_stats[int(k)] = {
                "mean_conf_exit": None,
                "min_conf_exit": None,
                "max_conf_exit": None,
                "num_exited": 0
            }

    # Split probs snapshot used during evaluation
    split_probs_list = split_probs.detach().cpu().tolist()

    # Edge slowdown (approx from profiles)
    edge_slowdown = profiles["block_0"]["edge_time_sec"] / profiles["block_0"]["cloud_time_sec"]

    # Network config from simulator
    net_avg_bw_mbps = getattr(net_sim, "avg_bw", None)
    net_avg_rtt_ms = getattr(net_sim, "avg_rtt", None)

    results = {
        "test_accuracy": accuracy,
        "avg_latency_ms": avg_latency,
        "p95_latency_ms": p95_latency,
        "exit_distribution": exit_counts,
        "exit_rates": exit_rates,
        "split_point": split_point,
        "split_block": int(split_block),
        "split_idx": int(split_idx),
        "split_probs": split_probs_list,
        "exit_thresholds": exit_thresholds,
        "exit_scales": exit_scales,
        "exit_confidence_stats": exit_conf_stats,
        "num_samples": total_samples,
        "dataset": dataset_name_lower,
        "edge_slowdown": edge_slowdown,
        "net_avg_bw_mbps": net_avg_bw_mbps,
        "net_avg_rtt_ms": net_avg_rtt_ms,
    }
    
    return results

============================================================
FILE: src\loss_function.py
============================================================
import torch
import torch.nn as nn
import numpy as np
from . import config as Config

class SLOAwareLoss(nn.Module):
    def __init__(self, profiles, network_sim, slo_target_sec=0.1, alpha_cvar=0.05, 
                 lambda_lat=Config.DEFAULT_LAMBDA_LAT, mu_slo=Config.DEFAULT_MU):
        super().__init__()
        self.profiles = profiles # The Loaded JSON
        self.net_sim = network_sim
        self.slo_target = slo_target_sec
        self.alpha_cvar = alpha_cvar # For P95 latency
        
        # Hyperparameters from paper
        self.lambda_lat = lambda_lat # Weight for average latency
        self.mu_slo = mu_slo         # Weight for SLO violation
        
        # Use per-sample CE so we can weight by exit probabilities
        self.ce_loss = nn.CrossEntropyLoss(reduction='none')
        self.epsilon = 1e-8 # Numerical stability for logs

    def forward(self, final_pred, exit_preds, split_probs, exit_confidences, targets, batch_network_state):
        """
        Calculates the 3-part loss from the paper.
        """
        bw, rtt = batch_network_state
        device = final_pred.device
        batch_size = final_pred.size(0)

        # --- PART 1: ACCURACY LOSS
         # Loss is weighted by the probability of actually exiting at each head.
        
        # Initialize with Final Head Loss
        # Probability of reaching final head = 1 - sum(p_exits)
        p_continue = torch.ones(batch_size, device=device)
        total_acc_loss = 0
        
        sorted_exits = sorted(exit_preds.keys())

        # Track expected exit probabilities (batch mean) for latency calculation later
        exit_prob_means = {}

        for idx_i, block_idx in enumerate(sorted_exits):
            pred = exit_preds[block_idx]
            p_exit = exit_confidences[block_idx] # Probability of exiting here
            
            # The probability of ACTUALLY exiting here is:
            # P(surviving previous exits) * P(exiting now) (survive all previous exits AND exit now)
            p_actual_exit = p_continue * p_exit # (B,)
            exit_prob_means[block_idx] = p_actual_exit.mean()
            
            # Per-sample CE
            ce_i = self.ce_loss(pred, targets)        # (B,)

            # Reward making the earliest exit good
            # (helps simpler datasets lean on early exits)
            head_weight = 1.5 if block_idx == sorted_exits[0] else 1.0
            total_acc_loss += (head_weight * p_actual_exit * ce_i).mean()
            
            # Update probability of continuing to next layer
            # Clamp to avoid numerical instability
            p_continue = p_continue * (1 - p_exit)
            
        # Add Final Head Loss
        final_ce  = self.ce_loss(final_pred, targets)
        p_final = p_continue
        p_final_mean = p_final.mean()

        total_acc_loss += (p_final * final_ce).mean()

        # --- PART 2 & 3: LATENCY & SLO 
        # Expected latency depends on:
        #   - which split we choose (split_probs)
        #   - which exit we actually take (exit probabilities)
        #
        # We approximate:
        #   E[T] = sum_s split_probs[s] *
        #              [ sum_exits p_exit_mean(e) * T(s, exit_at_block_e)
        #                + p_final_mean * T(s, exit_at_final) ]
        
        # 1. Calculate Latency for EVERY possible split point s
        block_keys = sorted(k for k in self.profiles.keys() if k.startswith("block_"))
        num_blocks = len(block_keys)
        
        device = split_probs.device

        edge_times = []
        cloud_times = []
        out_sizes = []
        
        for b in range(num_blocks):
            block_profile = self.profiles[f"block_{b}"]
            edge_times.append(block_profile["edge_time_sec"])
            cloud_times.append(block_profile["cloud_time_sec"])
            out_sizes.append(block_profile["output_bytes"])

        final_block = num_blocks - 1  # 4
        
        def latency_for_split_and_exit(s, exit_b):
            # 1) edge compute
            if exit_b <= s:
                # We exit on the edge before crossing the split
                t_edge = sum(edge_times[:exit_b + 1])
                t_comm = 0.0
                t_cloud = 0.0
            else:
                # Compute up to split on edge
                t_edge = sum(edge_times[:s + 1])
                # Comm at split
                data_size = out_sizes[s]
                t_comm = self.net_sim.estimate_transmission_time(data_size, bw, rtt)
                # Cloud compute until the exit block
                t_cloud = sum(cloud_times[s + 1:exit_b + 1])
            return t_edge + t_comm + t_cloud

        # Block index of each exit head and final head
        exit_blocks = sorted_exits                      # e.g. [1, 3]
        final_block = num_blocks - 1

        candidate_latencies = []
        num_splits = split_probs.size(0)  # e.g., 4 (split after 0,1,2,3)

        for s in range(num_splits):
            # Latency if we exit at each head given this split s
            total_t_s = 0.0

            # Early exits
            for block_idx in exit_blocks:
                p_mean = exit_prob_means[block_idx]     # scalar tensor
                t_e = latency_for_split_and_exit(s, block_idx)
                total_t_s += p_mean * t_e

            # Final exit (no early exit taken)
            t_final = latency_for_split_and_exit(s, final_block)
            total_t_s += p_final_mean * t_final

            candidate_latencies.append(total_t_s)

        candidate_latencies = torch.tensor(candidate_latencies, device=device, dtype=torch.float32)

        # Expected latency over splits
        expected_latency = torch.sum(split_probs * candidate_latencies)
        
        # --- SLO PENALTY (CVaR / ReLU) ---
        # Normalize latency by SLO so scales are stable across SLO values
        normalized_latency = expected_latency / (self.slo_target + self.epsilon)

        # Calculate violation: ReLU(Latency - Target)
        violation = torch.relu(normalized_latency - 1.0) ** 2
        
        # TOTAL LOSS
        # We verify that violation > 0 before applying penalty to avoid gradient noise
        total_loss = ( 
            total_acc_loss
            + (self.lambda_lat * normalized_latency)
            + (self.mu_slo * violation)
        )
                     
        return total_loss, expected_latency.item(), total_acc_loss.item()

============================================================
FILE: src\network_sim.py
============================================================
import numpy as np
from . import config as Config

class NetworkSimulator:
    def __init__(self, avg_bw_mbps=Config.DEFAULT_BW, std_bw=3.0, avg_rtt_ms=Config.DEFAULT_RTT, std_rtt=10):
        """
        Simulates network conditions.
        avg_bw_mbps: Average Bandwidth in Megabits per second
        avg_rtt_ms: Average Round Trip Time in milliseconds
        """
        self.avg_bw = avg_bw_mbps
        self.std_bw = std_bw
        self.avg_rtt = avg_rtt_ms
        self.std_rtt = std_rtt

    def sample_network_state(self):
        """
        Returns a sample (bandwidth_bps, rtt_sec)
        """
        # Sample Bandwidth (ensure it doesn't go below 0.1 Mbps)
        bw = np.random.normal(self.avg_bw, self.std_bw)
        bw = max(0.1, bw) 
        
        # Sample RTT (ensure it doesn't go below 1ms)
        rtt = np.random.normal(self.avg_rtt, self.std_rtt)
        rtt = max(1.0, rtt)
        
        # Convert to base units: bits per second and seconds
        bw_bps = bw * 1_000_000
        rtt_sec = rtt / 1000.0
        
        return bw_bps, rtt_sec

    def estimate_transmission_time(self, data_size_bytes, bw_bps, rtt_sec):
        """
        Calculates T_comm based on the paper's formula [cite: 112]
        T_comm = B_act / (bw/8) + alpha + beta * RTT
        """
        # Fixed overheads (alpha) and queuing delays (beta) 
        # assumed small constants for this simulation
        alpha = 0.005  # 5ms handshake overhead
        beta = 1.0     
        
        transmission_delay = data_size_bytes / (bw_bps / 8.0)
        t_comm = transmission_delay + alpha + (beta * rtt_sec)
        
        return t_comm

============================================================
FILE: src\resnet_split.py
============================================================
import torch
import torch.nn as nn
from torchvision.models import resnet18, ResNet18_Weights

class SplittableResNet18(nn.Module):
    def __init__(self, num_classes=10, input_channels=3, pretrained=True):
        """
        Args:
            num_classes (int): Number of output classes (e.g., 10 for FashionMNIST).
            input_channels (int): Number of input channels (1 for grayscale, 3 for RGB).
            pretrained (bool): If True, loads ImageNet weights.
        """
        super(SplittableResNet18, self).__init__()
        
        # 1. Load the base model with or without pretrained weights
        weights = ResNet18_Weights.DEFAULT if pretrained else None
        base_model = resnet18(weights=weights)
        
        # 2. Handle Input Channel Mismatch (e.g., FashionMNIST is 1 channel, ResNet expects 3)
        if input_channels != 3:
            old_conv = base_model.conv1
            
            # Create a new conv layer with correct input channels
            # We keep the same kernel size (7x7), stride (2), etc.
            new_conv = nn.Conv2d(
                in_channels=input_channels, 
                out_channels=old_conv.out_channels, 
                kernel_size=old_conv.kernel_size, 
                stride=old_conv.stride, 
                padding=old_conv.padding, 
                bias=old_conv.bias
            )
            
            if pretrained:
                # SMART INIT: Sum the weights across the RGB dimension to preserve 
                # learned patterns (edge detectors) instead of starting from scratch.
                # Shape is [Out, In, k, k]. Summing over dim 1 (Input channels).
                with torch.no_grad():
                    new_conv.weight[:] = torch.sum(old_conv.weight, dim=1, keepdim=True)
            
            base_model.conv1 = new_conv

        # 3. Handle Output Class Mismatch
        if base_model.fc.out_features != num_classes:
            base_model.fc = nn.Linear(base_model.fc.in_features, num_classes)

        # 4. Create the Blocks (same split logic as before)
        self.block0 = nn.Sequential(
            base_model.conv1,
            base_model.bn1,
            base_model.relu,
            base_model.maxpool
        )
        
        self.block1 = base_model.layer1
        self.block2 = base_model.layer2
        self.block3 = base_model.layer3
        self.block4 = base_model.layer4
        
        self.avgpool = base_model.avgpool
        self.fc = base_model.fc

        self.blocks = nn.ModuleList([
            self.block0, 
            self.block1, 
            self.block2, 
            self.block3, 
            self.block4
        ])

    def forward(self, x):
        for block in self.blocks:
            x = block(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

    def get_block_output_size(self, block_index, input_shape):
        """
        Helper to get the size of the tensor output by a specific block.
        Calculates dynamically based on the input_shape provided.
        """
        # Create a dummy input of the correct size/channels to trace dimensions
        dummy_input = torch.zeros(input_shape)
        
        with torch.no_grad():
            x = dummy_input
            for i, block in enumerate(self.blocks):
                x = block(x)
                if i == block_index:
                    # Return size in Bytes (float32 = 4 bytes)
                    return x.numel() * 4 
        return 0

============================================================
FILE: src\train_engine.py
============================================================
import torch
import torch.optim as optim
import os
import numpy as np

# Relative imports assuming this is run as a module or from root with path set
from .loss_function import SLOAwareLoss
from .data_utils import get_dataloaders

def train_model(config, profiles, net_sim, device, save_dir):
    # Unpack config
    batch_size = config['batch_size']
    epochs = config['epochs']
    slo_target = config['slo_target']
    
    # Data Setup
    train_loader, _, _, _ = get_dataloaders(
        dataset_name=config['dataset'],
        batch_size=batch_size,
    )

    # Model Setup
    model = config['model_instance']
    model.to(device)

    # Loss Setup
    criterion = SLOAwareLoss(
        profiles=profiles, 
        network_sim=net_sim, 
        slo_target_sec=slo_target,
        lambda_lat=config['lambda_lat'],
        mu_slo=config['mu_slo']
    )

    optimizer = optim.Adam(model.parameters(), lr=config['lr'])

    # Logging storage
    training_history = []

    print(f"\nStarting Training (SLO: {slo_target*1000:.1f}ms)...")
    print(f"{'Epoch':<6} | {'Acc':<8} | {'Avg Latency':<12} | {'Split Layer':<15} | {'Exit Probs (Avg)'}")
    print("-" * 85)

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        total_latency = 0
        correct = 0
        total_samples = 0
        
        # Track average exit probabilities per head for this epoch
        exit_prob_tracker = {k: [] for k in model.exit_points}

        # Anneal Temperature
        current_temp = config['temp_start'] - (epoch * (config['temp_start'] - config['temp_end']) / epochs)
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            network_state = net_sim.sample_network_state()
            
            optimizer.zero_grad()
            final_pred, exit_preds, split_probs, exit_confidences = model(data, temperature=current_temp)
            
            loss, exp_lat, acc_loss = criterion(
                final_pred, exit_preds, split_probs, exit_confidences, target, network_state
            )
            
            loss.backward()
            optimizer.step()
            
            # Metrics
            total_loss += loss.item()
            total_latency += exp_lat
            
            # Simple Accuracy (Final Head)
            pred = final_pred.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            total_samples += len(data)

            # Log exit probabilities
            for k, v in exit_confidences.items():
                exit_prob_tracker[k].extend(v.detach().cpu().numpy().tolist())

        # --- EPOCH STATS ---
        avg_acc = 100. * correct / total_samples
        avg_lat_ms = (total_latency / len(train_loader)) * 1000
        
        # Determine best split and full split distribution
        with torch.no_grad():
            split_probs_soft = torch.nn.functional.softmax(model.split_logits, dim=0)
            best_split = torch.argmax(split_probs_soft).item()
            split_conf = split_probs_soft[best_split].item()
            split_probs_list = split_probs_soft.detach().cpu().tolist()

            # Exit thresholds/scales per block index
            sorted_exits = sorted(model.exit_points)
            exit_thresholds = {}
            exit_scales = {}
            for idx, block_idx in enumerate(sorted_exits):
                exit_thresholds[int(block_idx)] = float(model.exit_threshold[idx].item())
                exit_scales[int(block_idx)] = float(model.exit_scale[idx].item())

        # Calculate Average Exit Probabilities (per head)
        avg_exit_probs = {}
        for k in sorted(exit_prob_tracker.keys()):
            if len(exit_prob_tracker[k]) > 0:
                avg_exit_probs[int(k)] = float(np.mean(exit_prob_tracker[k]))
            else:
                avg_exit_probs[int(k)] = 0.0

        probs_str = str([f"{avg_exit_probs[int(k)]:.2f}" for k in sorted(exit_prob_tracker.keys())])
        normalized_latency = (avg_lat_ms / (slo_target * 1000.0)) if slo_target > 0 else 0.0

        print(f"{epoch+1:<6} | {avg_acc:<7.1f}% | {avg_lat_ms:<9.2f} ms | Block {best_split} ({split_conf:.2f}) | {probs_str}")

        # Save logs
        log_entry = {
            "epoch": epoch + 1,
            "accuracy": avg_acc,
            "latency_ms": avg_lat_ms,
            "normalized_latency": normalized_latency,
            "split_decision": best_split,
            "split_confidence": split_conf,
            "split_probs": split_probs_list,
            "exit_probs_avg": avg_exit_probs,
            "exit_thresholds": exit_thresholds,
            "exit_scales": exit_scales,
            "loss": total_loss / len(train_loader)
        }
        training_history.append(log_entry)

    # Save Model
    model_path = os.path.join(save_dir, "model.pth")
    torch.save(model.state_dict(), model_path)
    
    return training_history

============================================================
FILE: baseline.py
============================================================
import argparse
import os
import json
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import subprocess
import sys
import time

# Local imports
import src.config as Config
from src.resnet_split import SplittableResNet18
from src.data_utils import get_dataloaders
from src.network_sim import NetworkSimulator

def run_command(command, description):
    """
    Helper to run a shell command and print status.
    """
    print(f"\n{'='*60}")
    print(f"STARTING: {description}")
    print(f"CMD: {' '.join(command)}")
    print(f"{'='*60}\n")
    
    start_time = time.time()
    try:
        # Check if python executable is correct
        full_command = [sys.executable] + command
        subprocess.run(full_command, check=True)
        duration = time.time() - start_time
        print(f"\n>>> SUCCESS: {description} completed in {duration:.2f}s")
    except subprocess.CalledProcessError as e:
        print(f"\n>>> ERROR: {description} failed with return code {e.returncode}")
        # Optional: Stop execution on error
        # sys.exit(1)

# ---------------------------
# 1. TRAINING (CE ONLY)
# ---------------------------

def train_ce_model(dataset_name, device, epochs, batch_size, lr):
    """
    Train a vanilla SplittableResNet18 on the given dataset using CE loss only.
    Returns: trained model, training_history (list of dicts)
    """
    train_loader, _, num_classes, input_channels = get_dataloaders(
        dataset_name=dataset_name,
        batch_size=batch_size,
    )

    # Always 3-channel input due to your transforms
    model = SplittableResNet18(
        num_classes=num_classes,
        input_channels=input_channels,  # should be 3
        pretrained=True
    ).to(device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    history = []

    print(f"\n=== Training baseline model on {dataset_name.upper()} ===")
    for epoch in range(1, epochs + 1):
        print(f"Epoch {epoch:02d}/{epochs}...")
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            outputs = model(data)
            loss = criterion(outputs, target)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * data.size(0)

            preds = outputs.argmax(dim=1)
            correct += preds.eq(target).sum().item()
            total += target.size(0)

        avg_loss = running_loss / total
        acc = 100.0 * correct / total
        print(f"Epoch {epoch:02d} | Loss: {avg_loss:.4f} | Acc: {acc:.2f}%")

        history.append({
            "epoch": epoch,
            "train_loss": avg_loss,
            "train_acc": acc,
        })

    return model, history


# ---------------------------
# 2. EVALUATION + LATENCY
# ---------------------------

def evaluate_baseline(model, dataset_name, profiles, device, net_sim, batch_size):
    """
    Evaluate accuracy and simulate latency for:
      - edge-only execution (all blocks on edge, no comm)
      - cloud-only execution (input -> cloud, all blocks on cloud)

    This mirrors the style of src/evaluate.py but without exits/splits.
    """
    _, test_loader, _, _ = get_dataloaders(
        dataset_name=dataset_name,
        batch_size=batch_size,
    )

    model.eval()
    total = 0
    correct = 0

    # Determine number of blocks and their times from profile
    block_indices = sorted(
        int(k.split("_")[1]) for k in profiles.keys() if k.startswith("block_")
    )
    edge_block_times = [profiles[f"block_{i}"]["edge_time_sec"] for i in block_indices]
    cloud_block_times = [profiles[f"block_{i}"]["cloud_time_sec"] for i in block_indices]

    edge_compute_total = float(sum(edge_block_times))
    cloud_compute_total = float(sum(cloud_block_times))

    latencies_edge = []
    latencies_cloud = []

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)

            # Forward pass (same for both paths; we only use profiles for latency)
            outputs = model(data)
            preds = outputs.argmax(dim=1)
            correct += preds.eq(target).sum().item()
            batch_size_actual = target.size(0)
            total += batch_size_actual

            # ---- Latency simulation ----
            # Per-batch network sample, like evaluate.py
            bw_bps, rtt_sec = net_sim.sample_network_state()

            # Size of input tensor per sample (float32)
            element_size = data.element_size()         # 4 bytes typically
            numel_per_sample = data[0].numel()
            input_size_bytes = element_size * numel_per_sample

            # EDGE-ONLY:
            # all blocks on edge; no network cost
            batch_edge_latency = edge_compute_total
            latencies_edge.extend([batch_edge_latency] * batch_size_actual)

            # CLOUD-ONLY:
            # send input once, then all blocks on cloud
            t_comm = net_sim.estimate_transmission_time(
                data_size_bytes=input_size_bytes,
                bw_bps=bw_bps,
                rtt_sec=rtt_sec,
            )
            batch_cloud_latency = cloud_compute_total + t_comm
            latencies_cloud.extend([batch_cloud_latency] * batch_size_actual)

    import numpy as np
    latencies_edge = np.array(latencies_edge)
    latencies_cloud = np.array(latencies_cloud)

    acc = 100.0 * correct / total

    results = {
        "dataset": dataset_name,
        "test_accuracy": acc,

        "edge_avg_latency_ms": float(latencies_edge.mean() * 1000.0),
        "edge_p95_latency_ms": float(np.percentile(latencies_edge, 95) * 1000.0),

        "cloud_avg_latency_ms": float(latencies_cloud.mean() * 1000.0),
        "cloud_p95_latency_ms": float(np.percentile(latencies_cloud, 95) * 1000.0),

        "num_samples": int(total),
        "edge_compute_total_sec": edge_compute_total,
        "cloud_compute_total_sec": cloud_compute_total,

        "edge_slowdown": float(edge_block_times[0] / (cloud_block_times[0] + 1e-8)),
        "net_avg_bw_mbps": getattr(net_sim, "avg_bw", None),
        "net_avg_rtt_ms": getattr(net_sim, "avg_rtt", None),
    }

    return results


# ---------------------------
# 3. MAIN SCRIPT: LOOP 3 DATASETS
# ---------------------------

def main():
    parser = argparse.ArgumentParser(
        description="Baseline CE-only ResNet18 on MNIST/FMNIST/CIFAR10 with edge vs cloud latency."
    )
    parser.add_argument("--epochs", type=int, default=Config.DEFAULT_EPOCHS)
    parser.add_argument("--batch_size", type=int, default=Config.DEFAULT_BATCH_SIZE)
    parser.add_argument("--lr", type=float, default=Config.DEFAULT_LR)

    parser.add_argument("--profile_file", type=str, default="latency_profile.json",
                        help="Latency profile JSON generated by profile_env.py")

    parser.add_argument("--bw_mbps", type=float, default=Config.DEFAULT_BW,
                        help="Average uplink bandwidth (for cloud-only comm).")
    parser.add_argument("--rtt_ms", type=float, default=Config.DEFAULT_RTT,
                        help="Average RTT (for cloud-only comm).")

    args = parser.parse_args()

    run_command(["profile_env.py"], f"Hardware Profiling for default EDGE_SLOWDOWN={Config.DEFAULT_SLOWDOWN}")
    if not os.path.exists(args.profile_file):
        raise FileNotFoundError(
            f"{args.profile_file} not found. Run profile_env.py first to generate it."
        )

    with open(args.profile_file, "r") as f:
        profiles = json.load(f)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    net_sim = NetworkSimulator(avg_bw_mbps=args.bw_mbps, avg_rtt_ms=args.rtt_ms)

    datasets = ["mnist", "fmnist", "cifar10"]

    base_dir = os.path.join("experiments", f"baseline_models")
    os.makedirs(base_dir, exist_ok=True)

    print(f"Saving baseline results under: {base_dir}")

    for ds in datasets:
        run_dir = os.path.join(base_dir, ds)
        os.makedirs(run_dir, exist_ok=True)

        # --- Train ---
        model, train_history = train_ce_model(
            dataset_name=ds,
            device=device,
            epochs=args.epochs,
            batch_size=args.batch_size,
            lr=args.lr,
        )

        # Save model
        model_path = os.path.join(run_dir, "model.pth")
        torch.save(model.state_dict(), model_path)

        # Save train log
        with open(os.path.join(run_dir, "train_log.json"), "w") as f:
            json.dump(train_history, f, indent=4)

        # --- Evaluate (accuracy + latency edge vs cloud) ---
        results = evaluate_baseline(
            model=model,
            dataset_name=ds,
            profiles=profiles,
            device=device,
            net_sim=net_sim,
            batch_size=args.batch_size,
        )

        with open(os.path.join(run_dir, "test_metrics.json"), "w") as f:
            json.dump(results, f, indent=4)

        print(f"\n[{ds.upper()}] Baseline results:")
        print(f"  Accuracy: {results['test_accuracy']:.2f}%")
        print(f"  Edge:  avg {results['edge_avg_latency_ms']:.2f} ms | p95 {results['edge_p95_latency_ms']:.2f} ms")
        print(f"  Cloud: avg {results['cloud_avg_latency_ms']:.2f} ms | p95 {results['cloud_p95_latency_ms']:.2f} ms")
        print("-" * 60)


if __name__ == "__main__":
    main()

============================================================
FILE: main.py
============================================================
import subprocess
import sys
import time
import os
import src.config as Config

def run_command(command, description):
    """
    Helper to run a shell command and print status.
    """
    print(f"\n{'='*60}")
    print(f"STARTING: {description}")
    print(f"CMD: {' '.join(command)}")
    print(f"{'='*60}\n")
    
    start_time = time.time()
    try:
        # Check if python executable is correct
        full_command = [sys.executable] + command
        subprocess.run(full_command, check=True)
        duration = time.time() - start_time
        print(f"\n>>> SUCCESS: {description} completed in {duration:.2f}s")
    except subprocess.CalledProcessError as e:
        print(f"\n>>> ERROR: {description} failed with return code {e.returncode}")
        # Optional: Stop execution on error
        # sys.exit(1)

def main(attempt_num = None):
    if attempt_num is None:
        print("Attempt number not provided. Set number in main.py __main__.")
        sys.exit(1)

    print("### AUTOMATED ABLATION RUNNER ###")
    print("This script will run Profiling + Ablation Studies")

    # 1. Hardware Profiling (Run once)
    run_command(["profile_env.py"], f"Hardware Profiling for default EDGE_SLOWDOWN={Config.DEFAULT_SLOWDOWN}")

    default_experiment = [
        {"name": "default", "slo": Config.DEFAULT_SLO, "rtt": Config.DEFAULT_RTT, "bw": Config.DEFAULT_BW, "data": Config.DEFAULT_DATASET, "mu": Config.DEFAULT_MU, "edge_slowdown": Config.DEFAULT_SLOWDOWN}, # Baseline
    ]

    # ABLATION A: SLO SENSITIVITY
    ablation_a = [
        {"name": "A_slo_100", "slo": 100, "rtt": Config.DEFAULT_RTT, "bw": Config.DEFAULT_BW, "data": Config.DEFAULT_DATASET, "mu": Config.DEFAULT_MU, "edge_slowdown": Config.DEFAULT_SLOWDOWN}, # Relaxed
        {"name": "A_slo_060",  "slo": 60, "rtt": Config.DEFAULT_RTT, "bw": Config.DEFAULT_BW, "data": Config.DEFAULT_DATASET, "mu": Config.DEFAULT_MU, "edge_slowdown": Config.DEFAULT_SLOWDOWN},
        {"name": "A_slo_040",  "slo": 40, "rtt": Config.DEFAULT_RTT, "bw": Config.DEFAULT_BW, "data": Config.DEFAULT_DATASET, "mu": Config.DEFAULT_MU, "edge_slowdown": Config.DEFAULT_SLOWDOWN}, # Stress Test
    ]

    # ABLATION B: NETWORK CONDITIONS
    ablation_b = [
        {"name": "B_net_fast",       "slo": Config.DEFAULT_SLO, "rtt": 10,  "bw": 50, "data": Config.DEFAULT_DATASET, "mu": Config.DEFAULT_MU, "edge_slowdown": Config.DEFAULT_SLOWDOWN}, # 5G
        {"name": "B_net_slow",       "slo": Config.DEFAULT_SLO, "rtt": 75,  "bw": 3,  "data": Config.DEFAULT_DATASET, "mu": Config.DEFAULT_MU, "edge_slowdown": Config.DEFAULT_SLOWDOWN}, # 3G
        {"name": "B_net_impossible", "slo": Config.DEFAULT_SLO, "rtt": 500, "bw": 1,  "data": Config.DEFAULT_DATASET, "mu": Config.DEFAULT_MU, "edge_slowdown": Config.DEFAULT_SLOWDOWN}, # Satellite
    ]

    # ABLATION C: DATASET DIFFICULTY
    ablation_c = [
        {"name": "C_data_mnist", "slo": Config.DEFAULT_SLO, "rtt": Config.DEFAULT_RTT, "bw": Config.DEFAULT_BW, "data": "mnist",   "mu": Config.DEFAULT_MU, "edge_slowdown": Config.DEFAULT_SLOWDOWN},
        {"name": "C_data_cifar10", "slo": Config.DEFAULT_SLO, "rtt": Config.DEFAULT_RTT, "bw": Config.DEFAULT_BW, "data": "cifar10", "mu": Config.DEFAULT_MU, "edge_slowdown": Config.DEFAULT_SLOWDOWN},
    ]

    # ABLATION D: EDGE SLOWDOWN SENSITIVITY
    ablation_d = [
        {"name": "D_edge_003",   "slo": Config.DEFAULT_SLO, "rtt": Config.DEFAULT_RTT, "bw": Config.DEFAULT_BW, "data": Config.DEFAULT_DATASET, "mu": Config.DEFAULT_MU, "edge_slowdown": 3.0},
        {"name": "D_edge_050",  "slo": Config.DEFAULT_SLO, "rtt": Config.DEFAULT_RTT, "bw": Config.DEFAULT_BW, "data": Config.DEFAULT_DATASET, "mu": Config.DEFAULT_MU, "edge_slowdown": 50.0},
        {"name": "D_edge_100", "slo": Config.DEFAULT_SLO, "rtt": Config.DEFAULT_RTT, "bw": Config.DEFAULT_BW, "data": Config.DEFAULT_DATASET, "mu": Config.DEFAULT_MU, "edge_slowdown": 100.0},
    ]

    # Combine all lists
    all_experiments = default_experiment + ablation_a + ablation_b + ablation_c + ablation_d


    # --- EXECUTION LOOP ---
    for i, exp in enumerate(all_experiments):
        # Default profile file
        profile_file = "latency_profile.json"

        # If this experiment is part of ablation D, re-profile with a custom slowdown
        edge_slow = exp["edge_slowdown"]
        if edge_slow != Config.DEFAULT_SLOWDOWN:
            edge_slow = exp["edge_slowdown"]
            profile_file = f"latency_profile_edge_{edge_slow}.json"

            # Run profiling for this slowdown
            run_command(["profile_env.py", "--edge_slowdown", str(edge_slow), "--output", profile_file],
                        f"Hardware Profiling for {exp['name']} (EDGE_SLOWDOWN={edge_slow})")
            
        
        cmd = [
            "run_experiment.py",
            "--exp_name", exp["name"],
            "--dataset", exp["data"],
            "--slo_ms", str(exp["slo"]),
            "--rtt_ms", str(exp["rtt"]),
            "--bw_mbps", str(exp["bw"]),
            "--mu_slo", str(exp["mu"]), 
            "--epochs", str(Config.DEFAULT_EPOCHS), # Standardized on 30 epochs for convergence
            "--edge_slowdown", str(edge_slow),
            "--profile_file", profile_file,
        ]
        
        step_desc = f"Experiment {i+1}/{len(all_experiments)}: {exp['name']}"
        run_command(cmd, step_desc)

    print("\n" + "="*60)
    print("ALL EXPERIMENTS COMPLETED.")
    print("Results are stored in the 'experiments/' directory.")

if __name__ == "__main__":
    main()

============================================================
FILE: profile_env.py
============================================================
import torch
import time
import json
import numpy as np
import argparse
from src.resnet_split import SplittableResNet18
import src.config as Config

def profile_hardware(filename="latency_profile.json", edge_slowdown=Config.DEFAULT_SLOWDOWN):
    # Since we are duplicating channels for MNIST/FMNIST, 
    # we ONLY profile for 3 channels (RGB) configuration.
    channels = 3
    input_shape = (1, channels, 32, 32)
    
    print(f"--- Profiling Hardware for 3-Channel Input ---")
    print(f"Edge slowdown factor: x{edge_slowdown:.1f}")
    
    # Init model with standard 3 channels
    model = SplittableResNet18(num_classes=10, input_channels=channels, pretrained=True)
    model.eval()
    
    dummy_input = torch.randn(input_shape)
    profiling_data = {}
    
    with torch.no_grad():
        # Warmup
        _ = model(dummy_input)
        
        x = dummy_input
        for i, block in enumerate(model.blocks):
            times = []
            for _ in range(100): # 50 runs for stability
                start = time.perf_counter()
                out = block(x)
                end = time.perf_counter()
                times.append(end - start)
            
            x = out
            median_time = np.median(times)
            output_size_bytes = model.get_block_output_size(i, input_shape)
            
            profiling_data[f"block_{i}"] = {
                "cloud_time_sec": median_time,
                "edge_time_sec": median_time * edge_slowdown, # Simulate slower edge
                "output_bytes": output_size_bytes
            }
            print(f"Block {i}: {median_time*1000:.3f}ms (Cloud) | Out: {output_size_bytes} B")

    with open(filename, "w") as f:
        json.dump(profiling_data, f, indent=4)
    print(f"Saved {filename}\n")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Profile hardware for edge/cloud ResNet blocks")
    parser.add_argument("--output", type=str, default="latency_profile.json",
                        help="Output JSON filename for latency profile.")
    parser.add_argument("--edge_slowdown", type=float, default=Config.DEFAULT_SLOWDOWN,
                        help="Factor by which edge is slower than cloud.")
    args = parser.parse_args()

    profile_hardware(filename=args.output, edge_slowdown=args.edge_slowdown)

============================================================
FILE: run_experiment.py
============================================================
import argparse
import os
import json
import torch
import sys

# Import from src
import src.config as Config
from src.resnet_split import SplittableResNet18
from src.deployment_model import DeploymentAwareResNet
from src.network_sim import NetworkSimulator
from src.train_engine import train_model
from src.evaluate import evaluate_model

def main():
    parser = argparse.ArgumentParser(description="Run Deployment-Aware Edge/Cloud Experiment")
    
    # Dataset Choice
    parser.add_argument("--dataset", type=str, default=Config.DEFAULT_DATASET, choices=["mnist", "fmnist", "cifar10"])
    
    # Experiment Settings
    parser.add_argument("--exp_name", type=str, default="default_run")
    parser.add_argument("--epochs", type=int, default=Config.DEFAULT_EPOCHS)
    parser.add_argument("--batch_size", type=int, default=Config.DEFAULT_BATCH_SIZE)
    parser.add_argument("--lr", type=float, default=Config.DEFAULT_LR)
    
    # SLO & Network Settings
    parser.add_argument("--slo_ms", type=float, default=Config.DEFAULT_SLO)
    parser.add_argument("--rtt_ms", type=float, default=Config.DEFAULT_RTT)
    parser.add_argument("--bw_mbps", type=float, default=Config.DEFAULT_BW)
    
    # Loss Weights
    parser.add_argument("--lambda_lat", type=float, default=Config.DEFAULT_LAMBDA_LAT)
    parser.add_argument("--mu_slo", type=float, default=Config.DEFAULT_MU)

    # Hardware profile / edge slowdown (for ablation E / logging)
    parser.add_argument("--profile_file", type=str, default="latency_profile.json")
    parser.add_argument("--edge_slowdown", type=float, default=Config.DEFAULT_SLOWDOWN)

    parser.add_argument("--attempt_num", type=str, default=Config.DEFAULT_ATTEMPT_NUM)
    
    args = parser.parse_args()

    # 1. Load Universal Profile
    profile_file = args.profile_file
    
    if not os.path.exists(profile_file):
        print(f"Error: {profile_file} not found. Run profile_env.py first or pass correct --profile_file.")
        sys.exit(1)

    print(f"Loading Universal Profile: {profile_file}")
    with open(profile_file, "r") as f:
        profiles = json.load(f)
        
    profiles['dataset_used'] = args.dataset

    # 2. Setup Directory
    run_dir = os.path.join("experiments", f"attempt_{args.attempt_num}", f"{args.exp_name}")
    os.makedirs(run_dir, exist_ok=True)
    print(f"Experiment Directory: {run_dir}")
    
    # Config
    config = {
        "dataset": args.dataset,
        "epochs": args.epochs,
        "batch_size": args.batch_size,
        "lr": args.lr,
        "slo_target": args.slo_ms / 1000.0,
        "avg_rtt": args.rtt_ms,
        "avg_bw": args.bw_mbps,
        "lambda_lat": args.lambda_lat,
        "mu_slo": args.mu_slo,
        "temp_start": 5.0,
        "temp_end": 0.1,
        "edge_slowdown": args.edge_slowdown,
    }
    
    with open(os.path.join(run_dir, "Config.json"), "w") as f:
        json.dump(config, f, indent=4)

    # 3. Init Resources
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    net_sim = NetworkSimulator(avg_bw_mbps=args.bw_mbps, avg_rtt_ms=args.rtt_ms)

    # Init Model
    # ALWAYS use input_channels=3 because we duplicate grayscale channels
    backbone = SplittableResNet18(num_classes=10, input_channels=3, pretrained=True)
    model = DeploymentAwareResNet(backbone, num_classes=10)
    
    config['model_instance'] = model

    # 4. Run Training
    print("--- Phase 1: Training ---")
    train_history = train_model(config, profiles, net_sim, device, run_dir)
    
    with open(os.path.join(run_dir, "train_log.json"), "w") as f:
        json.dump(train_history, f, indent=4)

    # 5. Run Evaluation
    print("\n--- Phase 2: Evaluation ---")
    test_results = evaluate_model(model, net_sim, profiles, device, dataset_name=args.dataset)
    
    with open(os.path.join(run_dir, "test_metrics.json"), "w") as f:
        json.dump(test_results, f, indent=4)

    print(f"\nExperiment Complete. Results saved in {run_dir}")

if __name__ == "__main__":
    main()